{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24cf08b-5e86-4d31-b489-21118b253640",
   "metadata": {},
   "source": [
    "# Laboratorio de Regresión Lineal\n",
    "\n",
    "Bienvenidos al laboratorio de Regresión Lineal utilizando el dataset de precios de casas en Boston. Este laboratorio está diseñado para ofrecer una comprensión práctica de cómo se implementa la regresión lineal en un escenario real de análisis de datos. A lo largo de este laboratorio, cargaremos y exploraremos el dataset de precios de casas de California, prepararemos los datos para el análisis, implementaremos la regresión lineal y evaluaremos el rendimiento de nuestro modelo.\n",
    "\n",
    "## Objetivos del Laboratorio\n",
    "\n",
    "- Cargar y explorar el dataset de precios de casas en Boston.\n",
    "- Limpiar y preparar los datos para la regresión lineal.\n",
    "- Analizar las correlaciones entre las diferentes características del dataset.\n",
    "- Aplicar la regresión lineal para predecir el valor de las casas.\n",
    "- Evaluar el rendimiento del modelo de regresión.\n",
    "\n",
    "Este laboratorio es una excelente oportunidad para desarrollar habilidades prácticas en regresión lineal y análisis de datos, lo que constituye una base esencial en el campo del aprendizaje automático.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e75b97-83cc-45e1-a234-804ffe741dbe",
   "metadata": {},
   "source": [
    "# Carga del Dataset de Viviendas de California\n",
    "\n",
    "Vamos a trabajar con el dataset de Viviendas de California, que es un conjunto de datos utilizado para predecir el valor mediano de las viviendas en diferentes distritos de California. Este dataset proporciona una serie de características que pueden influir en los precios de las viviendas, como la ubicación, el número de habitaciones, el ingreso medio de los habitantes, entre otros.\n",
    "\n",
    "## Columnas del Dataset\n",
    "\n",
    "El dataset de Viviendas de California incluye las siguientes características:\n",
    "\n",
    "- `longitude`: Longitud geográfica de la vivienda.\n",
    "- `latitude`: Latitud geográfica de la vivienda.\n",
    "- `housing_median_age`: Edad mediana de la vivienda.\n",
    "- `total_rooms`: Número total de habitaciones en el distrito.\n",
    "- `total_bedrooms`: Número total de dormitorios en el distrito.\n",
    "- `population`: Población del distrito.\n",
    "- `households`: Número de hogares en el distrito.\n",
    "- `median_income`: Ingreso medio de los hogares en el distrito.\n",
    "- `median_house_value`: Valor mediano de las viviendas ocupadas por sus propietarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad37cbd-f4b6-46da-857e-5c5623fec096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/_30q61fx5slbhg1f1kh3fhtc0000gn/T/ipykernel_88214/2658693849.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  median_house_value  \n",
      "0    -122.23               4.526  \n",
      "1    -122.22               3.585  \n",
      "2    -122.24               3.521  \n",
      "3    -122.25               3.413  \n",
      "4    -122.25               3.422  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargamos el dataset de Viviendas de California\n",
    "california_data = fetch_california_housing()\n",
    "data = pd.DataFrame(california_data.data, columns=california_data.feature_names)\n",
    "\n",
    "# Seleccionamos la variable median_house_value como el target de la regresion\n",
    "data['median_house_value'] = california_data.target\n",
    "\n",
    "# Mostramos las primeras filas para inspección\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf49c7-8df3-4114-bdd9-52e031611695",
   "metadata": {},
   "source": [
    "## Inserción de Valores Nulos\n",
    "\n",
    "El dataset de Viviendas de California originalmente no contiene valores nulos. Sin embargo, en situaciones del mundo real, es común enfrentarse a datasets con datos incompletos o perdidos. Para simular una situación más realista y practicar las técnicas de limpieza de datos, introduciremos valores nulos de forma aleatoria en nuestro dataset antes de proceder con la división en conjuntos de entrenamiento y prueba. Esta modificación nos permitirá luego aplicar métodos de imputación para manejar estos valores nulos de manera efectiva.\n",
    "\n",
    "**NO MODIFICAR ESTE BLOQUE DE CODIGO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2830d539-e60b-4300-8d9b-c55589367bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de valores nulos insertados: 3662\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definir el porcentaje de datos que deseamos convertir en nulos\n",
    "porcentaje_nulos = 0.02  # 2% de los datos\n",
    "\n",
    "# Calcular el número total de valores a convertir en nulos\n",
    "total_valores = data.shape[0] * data.shape[1]\n",
    "total_nulos = int(total_valores * porcentaje_nulos)\n",
    "\n",
    "# Generar índices aleatorios para insertar valores nulos\n",
    "indices_filas = np.random.randint(0, data.shape[0], total_nulos)\n",
    "indices_columnas = np.random.randint(0, data.shape[1], total_nulos)\n",
    "\n",
    "# Insertar valores nulos en el dataset\n",
    "for fila, columna in zip(indices_filas, indices_columnas):\n",
    "    data.iat[fila, columna] = np.nan\n",
    "\n",
    "# Verificación de los valores nulos insertados\n",
    "print(f'Total de valores nulos insertados: {data.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80bbf4-886a-4977-a343-9e36098eb7a1",
   "metadata": {},
   "source": [
    "\n",
    "## Separación en Conjuntos de Entrenamiento y Prueba\n",
    "\n",
    "Para el análisis, separaremos el dataset en un conjunto de entrenamiento y un conjunto de prueba. Esta práctica es esencial para poder entrenar el modelo con un conjunto de datos y luego evaluar su rendimiento con un conjunto diferente, lo que nos proporciona una evaluación objetiva de la capacidad predictiva del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3045af4c-6af1-48d8-9973-4f0dc749307b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 16512 registros\n",
      "Tamaño del conjunto de prueba: 4128 registros\n"
     ]
    }
   ],
   "source": [
    "# Dividimos el dataset en conjuntos de entrenamiento y prueba\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Mostramos la cantidad de registros en cada conjunto\n",
    "print(f'Tamaño del conjunto de entrenamiento: {train_data.shape[0]} registros')\n",
    "print(f'Tamaño del conjunto de prueba: {test_data.shape[0]} registros')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75367dc-0ab1-4409-91e5-ae77ad3d6e09",
   "metadata": {},
   "source": [
    "# Limpieza y Preparación de Datos\n",
    "\n",
    "Una vez que hemos cargado el dataset de Viviendas de California, el siguiente paso es preparar los datos para el análisis y modelado. Esto implica limpiar los datos, lo que puede incluir la eliminación o imputación de valores nulos, y normalizar o escalar las características para que el modelo de regresión lineal pueda interpretarlos de manera efectiva.\n",
    "\n",
    "## Limpieza de Datos\n",
    "\n",
    "La limpieza de datos es crucial para asegurar la calidad del análisis. Vamos a verificar si hay valores nulos en el dataset y decidir cómo manejarlos, ya sea eliminándolos o rellenándolos con un valor representativo como la media o la mediana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59d6cb0-971e-4bb5-8ac9-3e8a584314d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usted debe completar este codigo, recuerde que la limpieza debe ser sobre train_data y sobre test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ceb63a-21c5-4049-937e-b2a1783fb00c",
   "metadata": {},
   "source": [
    "# Normalización o Escalado de Datos\n",
    "\n",
    "La normalización o escalado de los datos es un paso crítico en el procesamiento de datos para el análisis de regresión lineal. Diferentes características del dataset pueden tener rangos de valores muy distintos. Por ejemplo, la población de un distrito puede variar entre miles o millones, mientras que la edad media de las viviendas puede estar entre 0 y 100 años. Si no se normalizan estos valores, una característica con un rango más amplio podría influir desproporcionadamente en el modelo y sesgar los resultados.\n",
    "\n",
    "## ¿Por qué normalizar o escalar?\n",
    "\n",
    "La normalización ayuda a garantizar que cada característica contribuya equitativamente al modelo de regresión, permitiendo una convergencia más rápida durante el entrenamiento y un modelo más equilibrado. Existen varias técnicas para escalar los datos, pero las más comunes son:\n",
    "\n",
    "- **Normalización Min-Max**: Escala y centra los datos entre 0 y 1.\n",
    "- **Estandarización Z-score**: Centra los datos en 0 con una desviación estándar de 1.\n",
    "\n",
    "La elección entre estas técnicas depende del análisis específico de los datos y de las preferencias del modelo.\n",
    "\n",
    "## Aplicación de la Normalización\n",
    "\n",
    "Vamos a aplicar la normalización a nuestro dataset de Viviendas de California para preparar los datos para la regresión lineal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970f6e8-00f7-48e9-b15f-37077cb37301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Escoger el escalador: MinMaxScaler para normalización o StandardScaler para estandarización\n",
    "scaler = StandardScaler()  # O usar MinMaxScaler()\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "scaled_data = scaler.fit_transform(train_data.drop('median_house_value', axis=1))\n",
    "features = train_data.drop('median_house_value', axis=1).columns\n",
    "train_data_scaled = pd.DataFrame(scaled_data, columns=features)\n",
    "train_data_scaled['median_house_value'] = train_data['median_house_value'].values\n",
    "\n",
    "\n",
    "# Opcional: mostrar gráficos para visualizar el efecto del escalado\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Comparación antes y después del escalado\n",
    "for i, col in enumerate(features):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.hist(train_data[col], alpha=0.5, label='Original', bins=20)\n",
    "    plt.hist(train_data_scaled[col], alpha=0.5, label='Scaled', bins=20)\n",
    "    plt.title(col)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2293a628-3cd0-4b1d-839d-c88f12da0e81",
   "metadata": {},
   "source": [
    "## Escalado del Conjunto de Prueba\n",
    "\n",
    "Es esencial aplicar el mismo escalado al conjunto de prueba que utilizamos en el conjunto de entrenamiento. Esto mantiene la consistencia y asegura que las predicciones del modelo sean válidas y comparables entre ambos conjuntos de datos. Aplicar el mismo escalador (ajustado con los datos de entrenamiento) al conjunto de prueba evita la fuga de información y garantiza que el modelo sea evaluado de manera justa.\n",
    "\n",
    "### Importancia de Usar el Mismo Escalador\n",
    "\n",
    "Utilizar el mismo escalador asegura que:\n",
    "\n",
    "- Mantenemos la integridad del proceso de evaluación del modelo, permitiendo que las predicciones en los datos de prueba sean comparables con el entrenamiento.\n",
    "- Evitamos el sesgo en el modelo que podría surgir al ajustar el escalador con información del conjunto de prueba.\n",
    "- Garantizamos que las transformaciones aplicadas sean coherentes en todo el conjunto de datos, lo cual es crucial para el rendimiento y la interpretación del modelo.\n",
    "\n",
    "### Visualización del Escalado\n",
    "\n",
    "Mostrar gráficos de los datos antes y después del escalado puede ayudar a visualizar cómo este proceso afecta la distribución de los datos en el conjunto de prueba, similar a lo que observamos en el conjunto de entrenamiento. Esto no solo confirma la aplicación correcta del escalado, sino que también proporciona una comprensión intuitiva de los cambios en los datos.\n",
    "\n",
    "En resumen, la aplicación adecuada del escalado a los conjuntos de entrenamiento y prueba es un paso crítico para preparar los datos para el análisis de regresión lineal y garantizar la validez de las evaluaciones del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1565b3e-26b2-40cc-933e-a306f53c84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que ya hemos ajustado el escalador al conjunto de entrenamiento como antes\n",
    "scaled_test_data = scaler.transform(test_data.drop('median_house_value', axis=1))\n",
    "test_data_scaled = pd.DataFrame(scaled_test_data, columns=features)\n",
    "test_data_scaled['median_house_value'] = test_data['median_house_value'].values\n",
    "\n",
    "\n",
    "# Opcional: mostrar gráficos para visualizar el efecto del escalado en el conjunto de prueba\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Comparación antes y después del escalado\n",
    "for i, col in enumerate(features):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.hist(test_data[col], alpha=0.5, label='Original', bins=20)\n",
    "    plt.hist(test_data_scaled[col], alpha=0.5, label='Scaled', bins=20)\n",
    "    plt.title(col)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac716b-3155-4cf9-9c14-b28ab05fcf20",
   "metadata": {},
   "source": [
    "# Análisis de Correlaciones entre los Datos\n",
    "\n",
    "En esta sección, realizaremos un análisis de correlaciones para entender cómo las diferentes características del dataset de Viviendas de California se relacionan entre sí y, lo más importante, cómo se relacionan con el precio mediano de las casas, que es nuestra variable objetivo.\n",
    "\n",
    "El análisis de correlaciones nos ayuda a identificar posibles relaciones lineales entre las variables. Una correlación cercana a 1 indica una fuerte relación positiva, mientras que una correlación cercana a -1 indica una fuerte relación negativa. Una correlación cercana a 0 sugiere que no hay una relación lineal significativa entre las variables.\n",
    "\n",
    "## Objetivos del Análisis de Correlaciones\n",
    "\n",
    "- Generar un mapa de calor para visualizar las correlaciones entre las características.\n",
    "- Identificar y seleccionar las características que tienen una correlación más fuerte con el precio mediano de las casas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd1b0761-3530-4e55-b15a-82a557aada8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usted debe completar este codigo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49deeb95-0810-4685-89bf-d37c04ae104f",
   "metadata": {},
   "source": [
    "# Construcción de las Matrices con las Principales Variables Correlacionadas\n",
    "\n",
    "Tras analizar las correlaciones, hemos identificado las tres características más correlacionadas con el precio de las casas: `MedInc`, `AveRooms` y `HouseAge`. Utilizaremos estas variables para construir nuestra matriz de características $X$ y avanzar con la regresión lineal.\n",
    "\n",
    "## Objetivos de esta Sección\n",
    "\n",
    "- Utilizar **las variables con mejor correlacion** para preparar la matriz de características $X$.\n",
    "- Añadir una columna de unos a $X$ para el término de intercepto.\n",
    "- Preparar el vector de salida $Y$ con los precios de las casas.\n",
    "- Aplicar la fórmula de la inversión de matriz para encontrar los coeficientes $\\theta$, utilizando la ecuación $\\theta = (X^T X)^{-1} X^T y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bade80-d2f8-44c5-aaf2-b1ccae7b46b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seleccionamos las variables con mayor correlación\n",
    "features_for_train = features #  aca puedes poner un arreglo con las features que seleccionaste luego del analisis de correlaciones.\n",
    "X = train_data[features_for_train]\n",
    "y = train_data['median_house_value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909c574-a59b-4533-8f21-92ac3cf9dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usted debe completar este codigo\n",
    "\n",
    "# Añadimos una columna de unos a X para el término de intercepto\n",
    "X = \n",
    "\n",
    "# Calculamos los coeficientes theta utilizando la fórmula de inversión de matriz\n",
    "theta = \n",
    "\n",
    "print('Coeficientes calculados para el modelo de regresión lineal usando las variables más correlacionadas:', theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c63fbb-a306-497c-a36f-9573d4318750",
   "metadata": {},
   "source": [
    "# Evaluación de la Regresión\n",
    "\n",
    "Después de construir el modelo de regresión lineal, es crucial evaluar su rendimiento utilizando el conjunto de prueba. Esto nos permite entender qué tan bien el modelo predice los precios de las viviendas fuera de la muestra de datos con la que fue entrenado.\n",
    "\n",
    "## Objetivos de esta Sección\n",
    "\n",
    "- Utilizar el conjunto de prueba para evaluar la regresión lineal encontrada.\n",
    "- Calcular y mostrar métricas clave de rendimiento, como el Error Cuadrático Medio (MSE) y el Coeficiente de Determinación (R²).\n",
    "\n",
    "Estas métricas nos proporcionarán una cuantificación clara del rendimiento del modelo. El MSE mide la media de los cuadrados de los errores, es decir, la diferencia cuadrática media entre los valores estimados y los reales. Por otro lado, R² indica la proporción de la variabilidad de la variable dependiente que ha sido explicada por el modelo de regresión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31b4ea-c6b1-4feb-b787-470278591c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Preparamos los datos de prueba\n",
    "test_X = test_data[features_for_train]\n",
    "test_y = test_data['median_house_value']\n",
    "test_X = np.c_[np.ones(test_X.shape[0]), test_X]\n",
    "\n",
    "# Predecimos los valores para el conjunto de prueba\n",
    "predicted_y = test_X @ theta\n",
    "\n",
    "# Calculamos las métricas de rendimiento\n",
    "mse = mean_squared_error(test_y, predicted_y)\n",
    "r2 = r2_score(test_y, predicted_y)\n",
    "\n",
    "print(f'Error Cuadrático Medio (MSE): {mse}')\n",
    "print(f'Coeficiente de Determinación (R²): {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d7ab1-66f3-4019-bfcf-a6d0ae14d358",
   "metadata": {},
   "source": [
    "# Evaluación de la Regresión: Interpretación de MSE y R²\n",
    "\n",
    "## Error Cuadrático Medio (MSE)\n",
    "\n",
    "El MSE es una métrica que mide la calidad del modelo en términos de capacidad predictiva.\n",
    "\n",
    "- **¿Qué mide?** El MSE calcula el promedio de los errores al cuadrado entre los valores predichos por el modelo y los valores reales.\n",
    "- **Interpretación de los valores:**\n",
    "  - **MSE = 0**: El modelo es perfecto, con predicciones exactas.\n",
    "  - **MSE alto**: Indica errores grandes en las predicciones, lo que puede señalar un modelo inadecuado o una alta variabilidad en los datos.\n",
    "- **Cómo interpretarlo:** Buscamos un MSE bajo, indicando menores errores en las predicciones. Es útil para comparar modelos en el mismo dataset, pero debe considerarse junto con la escala de los datos y otras métricas.\n",
    "\n",
    "## Coeficiente de Determinación (R²)\n",
    "\n",
    "El R² mide cuánta variabilidad en la variable dependiente puede explicarse con las variables independientes.\n",
    "\n",
    "- **¿Qué mide?** Proporción de la variabilidad en la variable dependiente explicada por el modelo.\n",
    "- **Interpretación de los valores:**\n",
    "  - **R² = 1**: El modelo explica toda la variabilidad.\n",
    "  - **R² = 0**: El modelo no explica la variabilidad.\n",
    "  - **R² negativo**: Indica que el modelo es peor que una línea horizontal simple.\n",
    "- **Cómo interpretarlo:** Un R² más alto sugiere un mejor ajuste del modelo a los datos. Sin embargo, un R² muy alto puede indicar sobreajuste, especialmente en modelos con muchas variables. Debe buscarse un equilibrio entre un R² alto y la complejidad del modelo.\n",
    "\n",
    "La interpretación de MSE y R² debe hacerse en contexto y considerando los objetivos específicos del análisis, utilizando estas métricas junto con otras herramientas de diagnóstico para evaluar completamente el rendimiento del modelo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
